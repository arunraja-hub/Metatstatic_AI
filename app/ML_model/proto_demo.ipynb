{"cells":[{"metadata":{},"cell_type":"markdown","source":"Protoype demo:\n1. Used simple CNN with dropout\n2. tensorflow+keras\n3. Current accuracy score of 0.8985 on test data(real_word)\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#imports\n\nfrom numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport datetime\nimport os\nimport cv2\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom time import gmtime, strftime\nstrftime(\"%Y-%m-%d %H:%M:%S\", gmtime())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7bbdd52c81188b8e9c528b88d9fd0da176bf4bc"},"cell_type":"code","source":"\nimage_size = 96\nchannels = 3\nsample_size = 50000 \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9c9f40ffab35044641b0dc7d9b18609af1aa25e"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train_labels.csv')\nprint(df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e18560bf69d3dfc0c4772e7c79bb119fd2eb634b"},"cell_type":"code","source":"df_train['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we want a equal number of samples in each class (0 and 1) to prevent class imbalance"},{"metadata":{"trusted":true,"_uuid":"270fc18640b552ecc3cb0e1dd3036441db7a4a2b"},"cell_type":"code","source":"df_0 = df_train[df_train['label'] == 0].sample(sample_size, random_state = 101)\ndf_1 = df_train[df_train['label'] == 1].sample(sample_size, random_state = 101)\ndf_train = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n# shuffling to ensure random distribution of labels, especially since we will be further dividing this train set into train and valid sets\ndf_train = shuffle(df_train)\n\ndf_train['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15ba9792e6a370b7560330af15b3cfe21185c1cb"},"cell_type":"code","source":"# further dividing training data into train and validation sets\ny = df_train['label']\n#90/10% split into train and val sets\ndf_train_train, df_train_val = train_test_split(df_train, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train_train.shape)\nprint(df_train_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now, df_train size is 90000, df_val_size is 10000 with equal distributuion of 0 and 1 labels in both sets"},{"metadata":{"trusted":true,"_uuid":"7de70d915a5f1d2599725e00bdb3b9103d947883"},"cell_type":"code","source":"df_train_train['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"392c0eea00be8e43a6e55438d1458650e842030b"},"cell_type":"code","source":"df_train_val['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a new directory of subdirs to easily feed data into models via generators"},{"metadata":{"trusted":true,"_uuid":"ff8acc2e92a1b1b5002d6e1bf9a1180c3256f19d"},"cell_type":"code","source":"# base dir\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n#base_dir structure\n# train_dir\n    # val0:no tumor\n    # val1:has tumor\n\n# val_dir\n    # val0:no tumor\n    # val1:has tumor\n\n#subpaths for subdirs\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\nval0 = os.path.join(train_dir, 'val0')\nos.mkdir(val0)\nval1 = os.path.join(train_dir, 'val1')\nos.mkdir(val1)\n\nval0 = os.path.join(val_dir, 'val0')\nos.mkdir(val0)\nval1 = os.path.join(val_dir, 'val1')\nos.mkdir(val1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e84c8a9642b030094b1888af3299063f883112a6"},"cell_type":"code","source":"# use id as index in the dataframe\ndf_train.set_index('id', inplace=True)\n\ntrain_list = list(df_train_train['id'])\nval_list = list(df_train_val['id'])\nprint(train_list[0:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"loading files into the directory created above using the index"},{"metadata":{"trusted":true,"_uuid":"afb8969a9ee75c13bddc808a4bcc326611baaaaf"},"cell_type":"code","source":"\n\nfor image in train_list:\n    #.tif is the image format\n    fname = image + '.tif'\n    target = df_train.loc[image,'label']\n\n    if target == 0:\n        label = 'val0'\n    if target == 1:\n        label = 'val1'\n        \n    src = os.path.join('../input/train', fname)\n    dst = os.path.join(train_dir, label, fname)\n    #copy\n    shutil.copyfile(src, dst)\n\n\nfor image in val_list:\n    #.tif is the image format\n    fname = image + '.tif'\n    target = df_train.loc[image,'label']\n\n    if target == 0:\n        label = 'val0'\n    if target == 1:\n        label = 'val1'\n\n    src = os.path.join('../input/train', fname)\n    dst = os.path.join(val_dir, label, fname)\n    #copy\n    shutil.copyfile(src, dst)\n    \n\n\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71532bfc32608289b1f773ffdbc8a7cea1bfb94c"},"cell_type":"code","source":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir/train_dir/val0')))\nprint(len(os.listdir('base_dir/train_dir/val1')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"897e9df543bb65b47bb00019dc681125ca08ee5d"},"cell_type":"code","source":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir/val_dir/val0')))\nprint(len(os.listdir('base_dir/val_dir/val1')))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"feed subdirs into generators"},{"metadata":{"trusted":true,"_uuid":"ef4fe7be09f11ff4badfd22d5fd5e03f8521ed58"},"cell_type":"code","source":"test_path = '../input/test'\n\ntrain_size = len(df_train_train)\nval_size = len(df_train_val)\ntrain_size_batch = 10\nval_size_batch = 10\n\ntrain_steps = np.ceil(train_size / train_size_batch)\nval_steps = np.ceil(val_size / val_size_batch)\nprint(train_steps)\nprint(val_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68fbd9d5fbb80859a82f94a12e335ce05a93bd51"},"cell_type":"code","source":"#adapted from https://keras.io/preprocessing/image/\ndatagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_gen = datagen.flow_from_directory(train_dir,\n                                        target_size=(image_size,image_size),\n                                        batch_size=train_size_batch,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(val_dir,\n                                        target_size=(image_size,image_size),\n                                        batch_size=val_size_batch,\n                                        class_mode='categorical')\n\n\ntest_gen = datagen.flow_from_directory(val_dir,\n                                        target_size=(image_size,image_size),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79da4d0a66a90cffe40580a596dd4d0e2bc45a9b"},"cell_type":"markdown","source":"model"},{"metadata":{"trusted":true,"_uuid":"b9835ea0fd0bca54138904895c39d38227a70c22"},"cell_type":"code","source":"kernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\n\n#layer 1\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\n#layer 2\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\n#layer 3\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\n#fully connected layer\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75cfc4fcb8dd3408d1c4fcf8cd85e0e2f5b611d7"},"cell_type":"markdown","source":"training"},{"metadata":{"trusted":true,"_uuid":"9de9715f49a63b55775b10abd2f461b395e23b5d"},"cell_type":"code","source":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"model.h5\"\n#saves model after every epoch\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n#reduces learning rate if there is no change in val_acc for 2 epochs, by factor of half \nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=20, verbose=1,\n                   callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa15a8afda3593973726e9087cbd98073041c908"},"cell_type":"markdown","source":"validation\n"},{"metadata":{"trusted":true,"_uuid":"428bdf5b24ff8cef35012205c3f2eb37006fc9e9"},"cell_type":"code","source":"# using best epoch on validation\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_train_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)\n\npredictions = model.predict_generator(test_gen, steps=len(df_train_val), verbose=1)\n#the validation set can be used to create a confusion matrix for presentation purposes#KIV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a10b51355f1aa8ae1f7148703791dc71f5f7e3e"},"cell_type":"markdown","source":"testing"},{"metadata":{"trusted":true,"_uuid":"c91e56e9c45caa655f7e45ec98ae0c492ce5358e"},"cell_type":"code","source":"shutil.rmtree('base_dir')\n\n#create test dir\n\n#feed test images to test_dir\ntest_dir = 'test_dir'\nos.mkdir(test_dir)\ntest_images = os.path.join(test_dir, 'test_images')\nos.mkdir(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f98c94ea8355a6d56d3f4a2791934b3365ed7865"},"cell_type":"code","source":"# move test images into image_dir\n\ntest_list = os.listdir('../input/test')\n    \n    \nfor image in test_list:\n    \n\n    fname = image\n#     fname=fname+'.tif'\n    src = os.path.join('../input/test', fname)\n    dst = os.path.join(test_images, fname)\n    #copy\n    shutil.copyfile(src, dst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6529facf2e4b60f4962fb0d08528e9f1ecd895a7"},"cell_type":"code","source":"test_path ='test_dir'\n\n\ntest_gen = datagen.flow_from_directory(test_path,\n                                        target_size=(image_size,image_size),\n                                        batch_size=1,\n                                        class_mode='binary',\n                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"150f61e5b959bcb2589d330652e3b1989caa35c4"},"cell_type":"code","source":"num_test_images = 57458\n\n# make sure we are using the best epoch\nmodel.load_weights('model.h5')\n\npredictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1346b4e4f22f053a5a70bef1f5edd1c2aee48404"},"cell_type":"code","source":"#inserting test preds into dataframe for submission csv\n\ndf_preds = pd.DataFrame(predictions, columns=['no_tumor_tissue', 'has_tumor_tissue'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff5fb65c49b505b405c726573dd9285664641bc1"},"cell_type":"code","source":"\ntest_filenames = test_gen.filenames\ndf_preds['file_names'] = test_filenames\n\ndf_preds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d04252577cd31825505fcbce69e7f3b6c052e35f"},"cell_type":"code","source":"\n#just want id, not .tif\ndef remove_id(x):\n    \n    # split into a list\n    a = x.split('/')\n    # split into a list\n    b = a[1].split('.')\n    removed_id = b[0]\n    \n    return removed_id\n\ndf_preds['id'] = df_preds['file_names'].apply(remove_id)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b29f14d5a29049a97d319eee2842d8c5ab3753e"},"cell_type":"code","source":"\ny_pred = df_preds['has_tumor_tissue']\n\nimage_id = df_preds['id']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"012b489902bf57b6e3097d9dc38a2513aa260b88"},"cell_type":"markdown","source":"submission to kaggle"},{"metadata":{"trusted":true,"_uuid":"429ab64aa5fec3e7652cd30e4c402f835264c8eb"},"cell_type":"code","source":"submission = pd.DataFrame({'id':image_id, \n                           'label':y_pred, \n                          }).set_index('id')\n\nsubmission.to_csv('results.csv', columns=['label']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6f0bdce1a5a18d6c045eda2b58abb92e089d47"},"cell_type":"code","source":"shutil.rmtree('test_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b789103dec6faf61148e291c59f9c7f13d1344a"},"cell_type":"code","source":"from time import gmtime, strftime\nstrftime(\"%Y-%m-%d %H:%M:%S\", gmtime())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}